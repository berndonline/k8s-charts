replicaCount: 2

image:
  repository: ghcr.io/berndonline/k8s/go-helloworld
  pullPolicy: IfNotPresent
  tag: "latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

podAnnotations: {}

podLabels:
  loki.logging.vector.dev/forwarding: enabled

podEnv:
  - name: RESPONSE
    value: "Hello, World - REST API!"
  - name: PORT
    value: "8080"
  - name: METRICSPORT
    value: "9100"

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: ""
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  hosts:
    - host: 
      paths:
        - path: /helloworld(/|$)(.*)
          pathType: ImplementationSpecific

metrics:
  enabled: false
  port: 9100
  endpoint: /metrics
  rbacEnabled: false
  prometheusServiceAccount: prometheus-k8s
  prometheusNamespace: monitoring
  rules:
    - name: helloworld-metrics
      rules:
      - alert: HelloWorldHighTotalConn
        annotations:
          description: Example helloworld Alert
          runbook_url: https://github.com/berndonline/go-helloworld/
          summary: helloworld is experiencing high connection rate.
        expr: |
          rate(http_requests_total[1m]) > 0.02
        for: 1m
        labels:
          severity: critical
      - alert: HelloWorldVersionMismatch
        annotations:
          description: Example helloworld Alert
          runbook_url: https://github.com/berndonline/go-helloworld/
          summary: helloworld version changed
        expr: |
          version > 0.1
        for: 1m
        labels:
          severity: warning
    - name: general.rules
      rules:
      - alert: TargetDown
        annotations:
          message: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.'
        expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job, namespace, service)) > 10
        for: 10m
        labels:
          severity: warning
      - alert: Watchdog
        annotations:
          message: |
            This is an alert meant to ensure that the entire alerting pipeline is functional.
            This alert is always firing, therefore it should always be firing in Alertmanager
            and always fire against a receiver. There are integrations with various notification
            mechanisms that send a notification when this alert is not firing. For example the
            "DeadMansSnitch" integration in PagerDuty.
        expr: vector(1)
        labels:
          severity: none

tracing:
  enabled: false
  collectorArgs: 
    - --reporter.grpc.host-port=dns:///jaeger-collector-headless.observability:14250
    - --reporter.type=grpc

resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}
